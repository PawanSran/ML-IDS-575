---
title: "Assignment-4"
author: "Pawanjeet Kaur"
date: "11/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question:1 Draw an example (of your own invention) of a partition of two- dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all as- pects of your figures, including the regions R1, R2, . . ., the cutpoints t1,t2,..., and so forth.

```{r question_1, echo=TRUE}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(0, 100), ylim = c(0, 100), xlab = "X", ylab = "Y")
# t1: x = 40; (40, 0) (40, 100)
lines(x = c(40, 40), y = c(0, 100))
text(x = 40, y = 108, labels = c("t1"), col = "red")
# t2: y = 75; (0, 75) (40, 75)
lines(x = c(0, 40), y = c(75, 75))
text(x = -8, y = 75, labels = c("t2"), col = "red")
# t3: x = 75; (75,0) (75, 100)
lines(x = c(75, 75), y = c(0, 100))
text(x = 75, y = 108, labels = c("t3"), col = "red")
# t4: x = 20; (20,0) (20, 75)
lines(x = c(20, 20), y = c(0, 75))
text(x = 20, y = 80, labels = c("t4"), col = "red")
# t5: y=25; (75,25) (100,25)
lines(x = c(75, 100), y = c(25, 25))
text(x = 70, y = 25, labels = c("t5"), col = "red")

text(x = (40 + 75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100 + 75)/2, labels = c("R2"))
text(x = (75 + 100)/2, y = (100 + 25)/2, labels = c("R3"))
text(x = (75 + 100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))

```

**DECISION TREE**

![](/Users/pawanjeetkaur/Downloads/Question1.jpg)


### Question 2: Consider the Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a functiono of pˆ(m-1) .The x-axis should display  pˆm ranging from 0 to 1, and the y-axis the value of the Gini index, classification error, and entropy.

```{r question_2 , echo=TRUE}
# Step 1: create probability vector 0 -1 with 0.01 gap 

p <- seq(0,1, 0.01)

ginni_index <- 2 * p * (1-p)

classification_error <- 1 - pmax(p, (1-p))

entropy <- - (p * log(p,2) + (1-p) * log((1-p),2))

# matplots are used to plot columns of a single matrix w.r.t single x values 

matplot(p , cbind(ginni_index, classification_error, entropy), 
        col = c("aquamarine4", "cyan4","darkred"),type = "b", xlab = "Probability", 
        ylab = "Ginni , Classification Error , Entropy", main = "Question 3 - Plots to show differnet metrics")
```

### Question 3: 
### (a) Split the data set into a training set and a test set.

```{r split_data, echo=TRUE}
library(ISLR)

attach(Carseats)
set.seed(1990)

library(caTools)
split = sample.split(Carseats,SplitRatio = 0.80)

train_data=subset(Carseats, split==TRUE)
test_data=subset(Carseats, split==FALSE)
```

### (b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain?

```{r reg_ques_3, echo=TRUE}
library(tree)
reg_tree <- tree(formula = Sales ~., data = train_data)

plot(reg_tree)
text(reg_tree, cex= .75)

test_predict <- predict(reg_tree, test_data)

mean_square_error <- mean((test_predict - test_data$Sales)^2)

mean_square_error
```

### (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r cv_ques_3, echo=TRUE}
cv_tree <- cv.tree(reg_tree)
plot(cv_tree$size ,cv_tree$dev ,type='b')

pruned_tree <- prune.tree(reg_tree, best=5)
plot(pruned_tree)
text(pruned_tree, cex = 0.75)

test_predict_2 <- predict(pruned_tree, test_data)
mean_square_error_2 <- mean((test_predict_2 - test_data$Sales)^2)

mean_square_error_2
```

**MSE increases after pruning the tree, so we use unpruned tree for further work**

### (d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to de- termine which variables are most important.

**Bagging is special case of random forest with m = p**

```{r bag_ques_3, echo=TRUE}
library(randomForest)

rf_tree <- randomForest(Sales ~. , data= train_data, mtry = ncol(train_data)-1 ,importance= TRUE)

plot(rf_tree)
summary(rf_tree)

test_predict_3 <- predict(rf_tree, test_data)
mean_square_error_3 <- mean((test_predict_3 - test_data$Sales)^2)

mean_square_error_3

importance(rf_tree)
varImpPlot(rf_tree)
```

### (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

**rf uses by default sqrt(p) as value of m for classification and p/3 for regression**
```{r rf_ques_3, echo=TRUE}
library(randomForest)

rf_tree_2 <- randomForest(Sales ~. , data= train_data, mtry = sqrt(ncol(train_data)) ,importance= TRUE)

plot(rf_tree_2)
summary(rf_tree_2)

test_predict_4 <- predict(rf_tree_2, test_data)
mean_square_error_4 <- mean((test_predict_4 - test_data$Sales)^2)

mean_square_error_4

importance(rf_tree_2)
varImpPlot(rf_tree_2)

error_list <- list()

for( i in seq(1: ncol(Carseats))) {
  rf_tree_2 <- randomForest(Sales ~. , data= train_data, mtry = i ,importance= TRUE)
  
  test_predict_4 <- predict(rf_tree_2, test_data)
  mean_square_error_4 <- mean((test_predict_4 - test_data$Sales)^2)
  
  error_list[i] <- mean_square_error_4
  importance(rf_tree_2)
}

length(error_list)
library(ggplot2)
df <- data.frame(seq(1:ncol(Carseats)), error_list)
plot(seq(1:ncol(Carseats)) ,y = error_list)
```

### Question 4: 

### (a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.

```{r processData_04, echo=TRUE}
library(ISLR)

attach(Hitters)
dim(Hitters)

new_data <- Hitters[!is.na(Salary),]
dim(new_data)

new_data$Salary <- log(new_data$Salary)
```

### (b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

```{r split_Data_04, echo=TRUE}
train_data <- new_data[1:200, ]
test_data <- new_data[-(1:200),]
```

### (c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.

```{r boost_datA_04, echo=TRUE}
library(gbm)

lambda_values <- c(c(), seq(0.002, 0.01, by=0.001))
lambda_values <- c(lambda_values, seq(0.02, 0.1, by=0.01))
lambda_values <- c(lambda_values, seq(0.2, 1, by=0.1))

train_errors <- rep(NA, length(lambda_values))
test_errors <- rep(NA, length(lambda_values))

for(i in 1:length(lambda_values)) {
  boost_model=gbm(Salary~.,data=train_data,distribution="gaussian",n.trees=1000, shrinkage = lambda_values[i])
  train_predict <- predict(boost_model , train_data, n.trees = 1000)
  test_predict <- predict(boost_model , test_data, n.trees = 1000)
  
  mean_square_train_error <- mean((train_predict - train_data$Salary)^2)
  train_errors[i] <- mean_square_train_error
  mean_square_test_error <- mean((test_predict - test_data$Salary)^2)
  test_errors[i] <- mean_square_test_error
}

library(ggplot2)

ggplot(data = data.frame(x = lambda_values ,y = train_errors) , aes(x=x, y=y)) + xlab("Shrinkage Parameter") + ylab("Train Errors") + geom_point()

```

### (d) Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.

```{r plot_04, echo=TRUE}
ggplot(data = data.frame(x = lambda_values ,y = test_errors) , aes(x=x, y=y)) + xlab("Shrinkage Parameter") + ylab("Test Errors") + geom_point()
```

### (e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.

```{r mse_comp_04, echo=TRUE}
#chapter 3 : Linear Regression Chapter 6: Ridge and Lasso
lm_model <- lm(formula = train_data$Salary~. , data = train_data)

lm_predict <- predict(lm_model, test_data)
mean_sq_err_lm <- mean((lm_predict - test_data$Salary)^2)

mean_sq_err_lm

# Lasso alpha=1 Ridge alpha = 0 
str(train_data)
library(glmnet)

x_reg_train <- as.matrix(train_data[, -c(14,15,20,19)])
y_reg_train <- as.matrix(train_data[, 19])

x_reg_test <- as.matrix(test_data[, -c(14,15,20,19)])
y_reg_test <- as.matrix(test_data[, 19])

ridge_model_val <- cv.glmnet(x_reg_train , y_reg_train, alpha = 0)
ridge_model_val
ridge_model <- glmnet(x_reg_train , y_reg_train, alpha = 0, lambda = ridge_model_val$lambda.min)
ridge_model
ridge_predict <- predict(ridge_model, x_reg_test)
mean_sq_err_ridge <- mean((ridge_predict - test_data$Salary)^2)

mean_sq_err_ridge

lasso_model <- glmnet(x_reg_train , y_reg_train, alpha = 1)
lasso_predict <- predict(lasso_model, x_reg_test)
mean_sq_err_lasso <- mean((lasso_predict - test_data$Salary)^2)

mean_sq_err_lasso
```

### (f) Which variables appear to be the most important predictors in the boosted model?

```{r impPredict_04, echo=TRUE}
best_boosted_model <- gbm(Salary~.,data=train_data,distribution="gaussian",
                          n.trees=1000, shrinkage =lambda_values[which.min(test_errors)])
summary(best_boosted_model)
```

### (g) Now apply bagging to the training set. What is the test set MSE for this approach?

```{r bag_04, echo=TRUE}
library(randomForest)

rf_bag_tree <- randomForest(Salary ~. , data= train_data, ntree= 100, mtry = ncol(train_data)-1 ,importance= TRUE)

plot(rf_bag_tree)
summary(rf_bag_tree)

test_predict_rf_bag <- predict(rf_bag_tree, test_data)
test_predict_rf_bag
mean_square_error_rf_bag <- mean((test_predict_rf_bag - test_data$Salary)^2)

mean_square_error_rf_bag
```

### Question 5: 
### (a) We are given n = 7 observations in p = 2 dimensions. For each observation, there is an associated class label.

```{r obser_05, echo=TRUE}
x1 <- c(3,2,4,1,2,4,4)
x2 <- c(4,2,4,4,1,3,1)

y <- c("Red","Red","Red","Red","Blue", "Blue", "Blue")

data <- data.frame(x1,x2,y)
plot(x1, x2, col = y, xlim = c(0,5), ylim = c(0,5))
```

### (b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)).

**Based on the graph points dividing two classes- (2,1.5) , (4,3.5)**
**Equation:  x2 - 3.5 = ((3.5 - 1.5)/(4-2)) (x1 - 4) => x2 -3.5 = x1 -4 => x2 = x1 - 0.5**

```{r drawline_05, echo=TRUE}
slope = 1
intercept = -0.5
plot(x1, x2, col = y, xlim = c(0,5), ylim = c(0,5))
abline(intercept, slope)

#library(e1071)

#svm.fit <- svm(formula=y~., data = data, kernel= "linear",scale= FALSE, method = "class")
#plot(svm.fit, data)
```

### (c) Describe the classification rule for the maximal margin classifier.It should be something along the lines of “Classify to Red if β0 + β1X1 + β2X2 > 0, and classify to Blue otherwise.” Provide the values for β0, β1, and β2.

**Answer β0 is intercept ( -0.5) and β1 -> 1 β2 <- -1 So equation : x1 - x2 - 0.5 < 0 Classify Red or else classify as blue**

### (d) On your sketch, indicate the margin for the maximal margin hyperplane
```{r classsketch_05, echo=TRUE}
plot(x1, x2, col = y, xlim = c(0,5), ylim = c(0,5))
abline(intercept, slope)
abline(intercept+0.5, slope, lty= 2)
abline(intercept-0.5, slope, lty=2)
```

### (e) Indicate the support vectors for the maximal margin classifier.

**The support vectors are the points (2,1), (2,2), (4,3) and (4,4)**

### (f) Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.

**Our 4th observation is at x = 4 and y =1 which is below our margin's defined. Hence, a little movement to that point will not effect our maximal margin hyperplane**

```{r marginmove_05, echo= TRUE}
x1_1 <- c(3,2,4,1,2,4,4.5)
x2_1 <- c(4,2,4,4,1,3,1.5)

y_1 <- c("Red","Red","Red","Red","Blue", "Blue", "Blue")

data_1 <- data.frame(x1_1,x2_1,y_1)
slope = 1
intercept = -0.5
plot(x1_1, x2_1, col = y_1, xlim = c(0,5), ylim = c(0,5))
abline(intercept, slope)
abline(intercept+0.5, slope, lty= 2)
abline(intercept-0.5, slope, lty=2)
```

**Above plot shows us that slight movement in last plot didnot make much difference on mazimal margin hyperplane**

### (g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.

Little swtich in the equation of hyperplane will give us the plane which is not the maximal margin plane.X1−X2−0.3=0 is one of plane which is not optimal plane.
```{r hyperplane_05, echo= TRUE}
plot(x1, x2, col = y, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.3, 1)
```


```{r svm_try_05, echo=TRUE}
#Using svm method, if we provide a cost to function which is the cost of violation of the margin.with cost argument small we get wide margins and with large value we get narrow margins.

#svm.fit_1 <- svm(formula=y~., data = data,type= 'C-classification', kernel= "linear",scale= FALSE, method = "class", cost=1.5)
#plot(svm.fit_1, data)
```

### (h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.

```{r last_05, echo=TRUE}
x1_1 <- c(3,2,4,1,2,4,4.5,2)
x2_1 <- c(4,2,4,4,1,3,1.5,3)

y_1 <- c("Red","Red","Red","Red","Blue", "Blue", "Blue","Blue")

slope = 1
intercept = -0.5
plot(x1_1, x2_1, col = y_1, xlim = c(0,5), ylim = c(0,5))
abline(intercept, slope)
```

